{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble은 정확도가 낮은 여러가지 모델들을 결합하여 더 높은 정확도를 이끌어내는데 쓰이는 모델이다.  \n",
    "Ensemble에는 bagging과 boosting이 포함되어 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting에는 GBM이라는 모델이 있다. 이 모델의 단점은 느리다는 것과 overfitting의 위험이 있다는 점이다.  \n",
    "그래서 이 단점들을 보완하기 위하여 XGBOOST라는 모델이 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST의 특징은 다음과 같다.\n",
    "1. gbm보다 빠르다\n",
    "2. overfitting 방지가 가능한 규제가 포함되어 있다.\n",
    "3. 분류와 회귀가 둘 다 가능하다.\n",
    "4. 조기 종료(early stopping)을 지원한다.\n",
    "- 아래의  두가지를 같이 설정해야 됨\n",
    "- eval_set: 성능평가를 수행할 평가용 데이터 세트 \n",
    "- eval_metric: 평가 세트에 적용할 성능 평가 방법(분류는 주로 error, logloss를 적용)\n",
    "5. Gradient boosting을 기반으로 하기에 앙상블 부스팅의 특징인 가중체 부여를 경사하강법으로 한다.\n",
    "6. 뛰어난 예측 성능\n",
    "7. 자체 내장된 교차검증\n",
    "- 반복 수행마다 내부적으로 교차검증 수행-최적화된 반복 수행횟수를 가질 수 있다.\n",
    "- 지정된 반복 횟수가 아니라 교차검증을 통해 평가 데이트세트의 평가 값이 최적화되면 반복을 중간에 멈출 수 있는 기능이 있다.\n",
    "8. 결측값 자체 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboostdml parameter\n",
    "1. 일반 파라미터\n",
    "\n",
    "- booster(디폴트는 gbtree): gbtree or gblinear 중 선택합니다.\n",
    "\n",
    "- nthread(디폴트는 전체 스레드를 사용): CPU 실행 스레드 개수입니다.\n",
    " \n",
    "\n",
    "2. 부스터 파라미터\n",
    "\n",
    "- eta(디폴트는 0.3): GBM에서 학습률과 같은 파라미터입니다. 보통은 0.01~0.2사이의 값을 선호합니다.\n",
    "\n",
    "- num_boost_rounds: GBM에서 n_estimators와 같은 파리미터입니다. \n",
    "- min_child_weight(디폴트는 1): 과적합을 조절하는데 사용이 되며 너무 큰값을 설정하게 되면 과적합이 될 수 있습니다.\n",
    "- gamma(디폴트는 0): 트리의 리프 노드를 추가적으로 나눌지 결정할 최소 손실 감소 값입니다. 값이 클수록 과적합 감소 효과가 있습니다. \n",
    "- max_depth(디폴트는 6): 트리 기반 알고리즘과 같은 파라미터입니다. 보통은 3~10 사이의 값을 사용하며 과적합 방지로 사용이 됩니다.\n",
    "- sub_sample(디폴트는 1): 데이터를 샘플링하는 비율을 정하여 줍니다. 보통 0.5~1사이의 값을 사용합니다.\n",
    "- colsample_bytree(디폴트는 1): 트리 생성에 필요한 피처를 임의로 샘플링 하는데 사용됩니다.\n",
    "- lambda(디폴트는 1): L2 Regularization 적용 값을 의미합니다. \n",
    "- alpha(디폴트는 0): L1 Regularization 적용 값을 의미합니다.\n",
    "- scale_pos_weight(디폴트는 1): 클래스가 불균형하게 이루어진 경우 균형을 유지하여 줍니다. \n",
    " \n",
    "\n",
    "3. 학습 태스크 파라미터\n",
    "\n",
    "- objective: 최솟값을 가져야할 손실 함수를 정의하여 줍니다. 이진 분류와 다중 분류에 따라 달라집니다. \n",
    "\n",
    "- binary:logistic: 이진 분류일 때 적용합니다.\n",
    "\n",
    "- multi:softmax: 다중 분류일 때 적용합니다. \n",
    "\n",
    "- multi:softprob: multi:softmax와 유사하지만 개별 레이블 클래스의 해당되는 예측 확률을 반환하여 줍니다.\n",
    "\n",
    "- eval_metric: 검증에 사용되는 함수를 정의하여 줍니다. 디폴트 값은 회귀:rmse, 분류: error 입니다.\n",
    "\n",
    "- rmse: Root Mean Square Error\n",
    "\n",
    "- mae: Mean Absolute Error\n",
    "\n",
    "- logloss: Negative log-likelihood\n",
    "\n",
    "- error: Binary classification error rate\n",
    "\n",
    "- merror: Multiclass classification error rate\n",
    "\n",
    "- mlogloss: Multiclass logloss\n",
    "\n",
    "- auc: Area under the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합이 심할 경우:\n",
    "1. eta값을 낮춘다. \n",
    "- eta값을 낮출 경우 num_estimators는 높여줘야 한다\n",
    "2. max_depth값을 낮춘다.\n",
    "3. min_child_weigh을 높인다\n",
    "4. gamma값을 높인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
